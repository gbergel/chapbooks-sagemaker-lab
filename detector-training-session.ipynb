{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_KEa0QwHRpd"
   },
   "outputs": [],
   "source": [
    "# Author  : Abhishek Dutta <adutta@robots.ox.ac.uk>\n",
    "# Date    : 2022-07-14\n",
    "#\n",
    "# Version History\n",
    "#     2022-07-25 : Workshop at ADHO Digital Humanities - 2022 (Tokyo) https://dh2022.adho.org/workshops-and-tutorials/wt-07\n",
    "#     2022-08-16 : JPNP: Adjustments to run in SageMaker Studio Labs for DH + BH conference 2022 https://dcsco-op.org/dhbh/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBjqwBBfvx_w"
   },
   "source": [
    "# Early Printed Book Illustration Detection Using Object Detectors\n",
    "\n",
    "In this tutorial, we describe the process to create a book illustration detector that can automatically detect illustrations in images containing early printed book pages. Such an illustration detector has enabled the [visual analysis of chapbooks printed in Scotland](https://www.robots.ox.ac.uk/~vgg/research/chapbooks/). The book illustration detector presented in this tutorial is trained using the [chapbooks dataset](https://data.nls.uk/data/digitised-collections/chapbooks-printed-in-scotland/) published in the public domain by the National Library of Scotland (NLS).\n",
    "\n",
    "This tutorial is organised as follows. First, we download and install all the required tools in this interactive python notebook. Next, we demonstrate an existing (i.e. pre-trained) illustration detector taken from the [VGG Chapbooks](https://www.robots.ox.ac.uk/~vgg/research/chapbooks/) project. The process of creating such an automatic illustration detector is described next. Finally, we describe some advanced, but optional, learning exercise which demonstrated the impact of the training sample on performance of automatic illustration detectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mREURnaaFuCJ",
    "tags": []
   },
   "source": [
    "## 1. Download and Install the Required Tools\n",
    "The illustration detector developed in the [VGG Chapbooks Project](https://www.robots.ox.ac.uk/~vgg/research/chapbooks/) is based on the [EfficientDet](https://github.com/google/automl/tree/master/efficientdet) object detector. The VGG Chapbooks Project code repository contains all the data, pre-trained object detector and tools required in this tutorial. Therefore, we download the [code repository](https://gitlab.com/vgg/nls-chapbooks-illustrations/) and setup the environment in this Jupyter (Sagemaker Studio Lab) document. This setup is essential for all the remaining sections of this tutorial and therefore must be executed before running commands from any other section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependancies into the Sagemaker environment (up to 5 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%conda install tensorflow opencv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install project code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xOJQc3VuBep1",
    "outputId": "a3445681-fbcd-4223-c1d7-e6ed45712dc1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "## Download VGG Chapbooks project code repository and setup environment\n",
    "import os\n",
    "import sys\n",
    "import tensorflow.compat.v1 as tf\n",
    "import cv2\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "BASEDIR = %pwd\n",
    "\n",
    "if 'nls-chapbooks-illustrations' not in os.getcwd():\n",
    "  !git clone --recurse-submodules https://gitlab.com/vgg/nls-chapbooks-illustrations.git\n",
    "  os.chdir('nls-chapbooks-illustrations/automl/efficientdet')\n",
    "  !git pull origin master  # update EfficientDet code to the latest version\n",
    "  %pip install -r requirements.txt\n",
    "  %pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
    "\n",
    "## Errors building wheel for cocoapi\n",
    "\n",
    "## Create folders used to store data (images, annotations, etc.) for this tutorial\n",
    "DATA_DIR = BASEDIR + '/sample_data/chapbooks/'\n",
    "DEMO_DIR = os.path.join(DATA_DIR, 'demo')\n",
    "DET_DIR = os.path.join(DATA_DIR, 'demo', 'detection-results')\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "\n",
    "if not os.path.exists(DET_DIR):\n",
    "  os.makedirs(DET_DIR)\n",
    "if not os.path.exists(TRAIN_DIR):\n",
    "  os.makedirs(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a utility function to show images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xOJQc3VuBep1",
    "outputId": "a3445681-fbcd-4223-c1d7-e6ed45712dc1"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## We define a utility function that will be used throughout this tutorial\n",
    "def show_thumbnail(img_fn, tsize=500):\n",
    "  '''\n",
    "  Show a thumbnail sized version of an image in Colab\n",
    "  '''\n",
    "  img = cv2.imread(img_fn)\n",
    "  w, h, c = img.shape\n",
    "  if w > tsize or h > tsize:\n",
    "    if w > h:\n",
    "      new_width = tsize\n",
    "      new_height = int( (w/h) * new_width )\n",
    "    else:\n",
    "      new_height = tsize\n",
    "      new_width = int( (h/w) * new_height )\n",
    "\n",
    "    resized_img = cv2.resize( img, (new_width, new_height) )\n",
    "    img2 = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB)\n",
    "    #cv2_imshow(resized_img)\n",
    "\n",
    "  else:\n",
    "    img2 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "  # Use pyplot instead of cv2_imgshow\n",
    "  #  https://gist.github.com/mstfldmr/45d6e47bb661800b982c39d30215bc88\n",
    "  #  https://stackoverflow.com/questions/36367986/\n",
    "  plt.figure(figsize=(15,15))\n",
    "  plt.imshow(img2)\n",
    "  plt.xticks([]), plt.yticks([])\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYIgGRKf9U5Y"
   },
   "source": [
    "## 2. Demo of an Automatic Book Illustration Detector\n",
    "\n",
    "In this section, we demonstrate the automatic illustration detection capabilities developed in the [VGG Chapbooks](https://www.robots.ox.ac.uk/~vgg/research/chapbooks/) project. First, we download a [test image](https://gitlab.com/vgg/nls-chapbooks-illustrations/-/blob/master/data/images/test_images/BL_compultensian-polyglot-bible-g_11955_title_page.jpg). It is possible to chose a different test image by enter the URL of that image in the text input box shown in the right hand side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921
    },
    "id": "bhiphZthR1b3",
    "outputId": "0ae5051b-101b-4b78-844f-53e1ddaa5e03"
   },
   "outputs": [],
   "source": [
    "## Download test image\n",
    "image_url =  'https://gitlab.com/vgg/nls-chapbooks-illustrations/-/raw/master/data/images/test_images/BL_tyndales-new-testament-1526-c_188_a_17_f001r.jpg'#@param\n",
    "test_image_filename = 'test_image.jpg'\n",
    "test_image_path = os.path.join(DEMO_DIR, 'test_image.jpg')\n",
    "!wget {image_url} -O {test_image_path}\n",
    "\n",
    "show_thumbnail(test_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rc-GKsvu6FLU"
   },
   "source": [
    "Next, we apply the pretrained Illustration Detector to this test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V-4EScKuELlD",
    "outputId": "09ad926a-76b3-4913-83f3-39ef717345f0"
   },
   "outputs": [],
   "source": [
    "## Apply illustration detector to test image\n",
    "os.chdir(BASEDIR + '/nls-chapbooks-illustrations/tools')\n",
    "!PYTHONPATH={BASEDIR}/nls-chapbooks-illustrations/automl/efficientdet/ python detect-illustration.py \\\n",
    "  --model-name=efficientdet-d0 \\\n",
    "  --saved-model-dir={BASEDIR}/nls-chapbooks-illustrations/data/efficientdet/saved_model/v1/  \\\n",
    "  --hparams={BASEDIR}/nls-chapbooks-illustrations/data/efficientdet/hparams.yaml \\\n",
    "  --input-image={test_image_path} \\\n",
    "  --output-image-dir={DET_DIR} \\\n",
    "  --output-json-fn={DET_DIR}/metadata.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-mPACle6ReL"
   },
   "source": [
    "Finally, we visualise the detection results and show the confidence (a value of 1.0 implies 100% confidence) of these detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zGsAZWSS6UzH",
    "outputId": "dc692555-2b75-4142-b674-7f9772dbf54d"
   },
   "outputs": [],
   "source": [
    "## Show detection results\n",
    "!ls -l {DET_DIR}\n",
    "show_thumbnail( os.path.join(DET_DIR, 'test_image.jpg'), tsize=800 )\n",
    "with open( os.path.join(DET_DIR, 'metadata.json'), 'r' ) as f:\n",
    "  d = json.load(f)\n",
    "  print( json.dumps(d, indent=4) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVdOc1Jz_a5C"
   },
   "source": [
    "## 3. Creating an Automatic Book Illustration Detector?\n",
    "In this section, we describe the process of creating an automatic book illustration detector which involves creating manually annotated examples of the object (i.e. book illustrations) and a training process in which an object detector learns to identify these objects in an image using the manually annotated samples. The training process is fully automatic. Therefore, the only laborious part of this process is the manual annotation of object instances. To reduce the workload, we have provided samples of manual annotations and learners are required to only manually annotate 5 images. The process is described below.\n",
    "\n",
    "### 3.1 Create Manually Annotated Dataset\n",
    "To train an object detector, we need examples of how the object appears in an image. Since we are creating a book illustration detector, we collect some images of book pages containing an illustration and manually annotate (i.e. draw a rectangular box) the location of these illustrations.\n",
    "\n",
    "For this tutorial, we have prepared a set of 25 images that contains an illustration and are taken from the NLS Chapbooks Dataset. Here are the steps to view and create the required manual annotations.\n",
    "\n",
    "1. Download the [nls-chapbooks-25.zip (9MB)](https://www.robots.ox.ac.uk/~vgg/research/chapbooks/dh2022/data/nls-chapbooks-25.zip) file which contains the following.\n",
    "\n",
    "  * 25 images from Chapbooks for training in `img/` folder\n",
    "  * Manual annotations of **only 20 training images** in `train.json` file (remaining 5 manual annotations should to be done by the learner)\n",
    "  * 50 images from Chapbooks for testing in `img/` folder\n",
    "  * Manual annotations of all 50 test images in `test.json` file\n",
    "  * List Annotator (LISA) application `lisa.html` to create new manual annotations and view existing annotations\n",
    "\n",
    "2. Open `lisa.html` file in a web browser such as Firefox or Chrome. Safari and Internet Explorer are not recommended.)\n",
    "\n",
    "3. Click \"Browse\" (or Choose File) in the \"Load Existing Project\" section and select the training annotations contained in the `train.json` LISA project.\n",
    "\n",
    "4. Draw a rectangular bounding box around illustration of 5 chapbook images that are missing manual annotations. To draw a bounding box around an illustration, press your mouse or trackpad button and drag your pointer over the illustration.\n",
    "\n",
    "5. After all the manual annotations are created, press `Ctrl` + `S` (i.e. hold Control key and press the `S` key) and save the annotations as `train25.json` in the same folder as the 'nls-chapbooks-25' folder that you previously downloaded. .\n",
    "\n",
    "```\n",
    "\n",
    "The manual annotations are complete. Let us now prepare the training and testing image dataset in this environment. We first download a copy of [nls-chapbooks-25.zip](https://www.robots.ox.ac.uk/~vgg/research/chapbooks/dh2022/data/nls-chapbooks-25.zip) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJ2GXJKkCFkp",
    "outputId": "99969009-8c5e-4cbb-e05e-4fcce4689446"
   },
   "outputs": [],
   "source": [
    "os.chdir(TRAIN_DIR)\n",
    "if not os.path.exists( os.path.join(TRAIN_DIR, 'nls-chapbooks-25.zip') ):\n",
    "  !wget https://www.robots.ox.ac.uk/~vgg/research/chapbooks/dh2022/data/nls-chapbooks-25.zip\n",
    "  !unzip nls-chapbooks-25.zip\n",
    "!ls nls-chapbooks-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foUD9H7WIiV-"
   },
   "source": [
    "The `train.json` file extracted from the [nls-chapbooks-25.zip](https://www.robots.ox.ac.uk/~vgg/research/chapbooks/dh2022/data/nls-chapbooks-25.zip) file contains manual annotations for only 20 images in the training dataset. You can now upload the `train25.json` file that you had saved earlier to this environment. Click on the File Browser (folder icon) on the left hand side panel of this notebook. In the folder tree view, click \"sample_data -> chapbooks -> train -> nls-chapbooks-25\". Now, click on the Upload Files button above this pane . Now point to the `train25.json` file that you had saved earlier in your local computer. To check if the upload was successful, run the following command and ensure that one of the listing entries corresponds to `train25.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RUt05KJzKY6F",
    "outputId": "e886cc7f-a529-43dd-828f-17f647e4114d"
   },
   "outputs": [],
   "source": [
    "## Ensure that the user uploaded train25.json file has been placed correctly\n",
    "!ls -l {TRAIN_DIR}/nls-chapbooks-25\n",
    "if not os.path.exists( os.path.join(TRAIN_DIR, 'nls-chapbooks-25', 'train25.json') ):\n",
    "  raise ValueError('Error: you missed to upload the train25.json file.\\nClick \"Files\" in the left toolbar and upload file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELGE9wvMzGX6"
   },
   "source": [
    "### 3.2 Convert Annotations\n",
    "\n",
    "The manual annotations of bounding boxes corresponding to illustrations are contained in `train25.json` file and the corresponding images are stored in `img` folder. Manual annotations for the test set are already contained in `test.json` file. We can now export the annotations to [COCO](https://cocodataset.org/#format-data) format which is the most commonly used format for training object detectors, including our EfficientDet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nCLL2RORyVjn",
    "outputId": "90d61535-6f15-44c8-c6f3-cfee78a03e1d"
   },
   "outputs": [],
   "source": [
    "## Convert manual annotations to COCO format\n",
    "os.chdir(BASEDIR + '/nls-chapbooks-illustrations/tools')\n",
    "!python lisa_to_coco.py --lisa_project_fn={TRAIN_DIR}/nls-chapbooks-25/train25.json\n",
    "!python lisa_to_coco.py --lisa_project_fn={TRAIN_DIR}/nls-chapbooks-25/test.json\n",
    "\n",
    "## Expected output\n",
    "# Exporting annotations in 25 images to COCO format\n",
    "# ...\n",
    "# Written COCO dataset to /content/sample_data/chapbooks/train/nls-chapbooks-25/train25_train_coco.json\n",
    "# Exporting annotations in 50 images to COCO format\n",
    "# ...\n",
    "# Written COCO dataset to /content/sample_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqAz31W70wwH"
   },
   "source": [
    "The program code for training EfficientDet object detector model uses the [tfrecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) data storage format to represent images and their manual annotations in a compact form. Therefore, we convert our annotations in [COCO](https://cocodataset.org/#format-data) format to the tfrecord format using the [create_coco_tfrecord.py](https://github.com/google/automl/blob/master/efficientdet/dataset/create_coco_tfrecord.py) script so that it can be used for EfficientDet training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIGwCPJzzRWP",
    "outputId": "3b0707ac-c3e1-485a-9997-137ed50539cf"
   },
   "outputs": [],
   "source": [
    "## Convert to tfrecord format\n",
    "TFRECORD_DIR = os.path.join(TRAIN_DIR, 'tfrecord', 'nls-chapbooks-25')\n",
    "if not os.path.exists(TFRECORD_DIR):\n",
    "  os.makedirs(TFRECORD_DIR)\n",
    "os.chdir(BASEDIR + '/nls-chapbooks-illustrations/automl/efficientdet/')\n",
    "!PYTHONPATH={BASEDIR}/nls-chapbooks-illustrations/automl/efficientdet/ python \\\n",
    "  dataset/create_coco_tfrecord.py \\\n",
    "  --logtostderr \\\n",
    "  --image_dir={TRAIN_DIR}/nls-chapbooks-25/img/ \\\n",
    "  --object_annotations_file={TRAIN_DIR}/nls-chapbooks-25/train25_train_coco.json \\\n",
    "  --output_file_prefix={TFRECORD_DIR}/train \\\n",
    "  --num_shards=1\n",
    "\n",
    "!PYTHONPATH={BASEDIR}/nls-chapbooks-illustrations/automl/efficientdet/ python \\\n",
    "  dataset/create_coco_tfrecord.py \\\n",
    "  --logtostderr \\\n",
    "  --image_dir={TRAIN_DIR}/nls-chapbooks-25/img/ \\\n",
    "  --object_annotations_file={TRAIN_DIR}/nls-chapbooks-25/test_train_coco.json \\\n",
    "  --output_file_prefix={TFRECORD_DIR}/test \\\n",
    "  --num_shards=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_q5Ku0wfRzvT"
   },
   "source": [
    "The next command is used to confirm that we have the following two files in the `/sample_data/chapbooks/train/tfrecord/nls-chapbooks-25/` folder.\n",
    "```\n",
    "test-00000-of-00001.tfrecord\n",
    "train-00000-of-00001.tfrecord\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Du027WySS3e",
    "outputId": "3951ca77-7903-4fd2-816b-67378f6602df"
   },
   "outputs": [],
   "source": [
    "!ls -l {TRAIN_DIR}/tfrecord/nls-chapbooks-25/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6Pv1fYQ1C70"
   },
   "source": [
    "### 3.3 Train Object Detector Using Manually Annotated Dataset\n",
    "\n",
    "Now we can start the training of our EfficientDet object detector using the 25 manually annotated pages. We will use the test dataset -- containing 50 manually annotated instances -- to evaluate the performance of our final trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MidxpPu11OXY",
    "outputId": "4b939196-ff46-4b43-d55f-b76ee824ce3e"
   },
   "outputs": [],
   "source": [
    "## Start Training Process\n",
    "MODEL_BASE_DIR = os.path.join(TRAIN_DIR, 'model')\n",
    "if not os.path.exists(MODEL_BASE_DIR):\n",
    "  os.makedirs(MODEL_BASE_DIR)\n",
    "MODEL_DIR = os.path.join(MODEL_BASE_DIR, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "  os.makedirs(MODEL_DIR)\n",
    "\n",
    "os.chdir(BASEDIR + '/nls-chapbooks-illustrations/automl/efficientdet/')\n",
    "if not os.path.exists('efficientdet-d0'):\n",
    "  !wget  https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/efficientdet-d0.tar.gz\n",
    "  !tar zxf efficientdet-d0.tar.gz\n",
    "\n",
    "!PYTHONPATH={BASEDIR}/nls-chapbooks-illustrations/automl/efficientdet/ python \\\n",
    "  main.py --mode=train \\\n",
    "  --train_file_pattern={TFRECORD_DIR}/train-*-of-00001.tfrecord \\\n",
    "  --val_file_pattern={TFRECORD_DIR}/test-*-of-00001.tfrecord \\\n",
    "  --model_name=efficientdet-d0 \\\n",
    "  --model_dir={MODEL_DIR}  \\\n",
    "  --ckpt=efficientdet-d0 \\\n",
    "  --train_batch_size=8 \\\n",
    "  --num_examples_per_epoch=25 --num_epochs=15  \\\n",
    "  --hparams=\"num_classes=1,moving_average_decay=0\" \\\n",
    "  --eval_after_train=True --tf_random_seed=9973\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMrMgywb5e3f"
   },
   "source": [
    "The training process (15 epochs) takes around 7 minutes. After the training is complete, the trained book illustration detector is automatically evaluated on our test dataset (50 manually annotated instances that were not present in the training set). This provides a reasonable estimate of the performance of this model when it is applied to unseen book images.\n",
    "\n",
    "We will use the following two metrics to assess the performance of the trained model: Average Precision (AP) and Average Recall (AR). A higher value of precision implies that the detections were closer to the ground truth (i.e. the location of book illustrations). A higher recall value implies that most of the book illustrations were detected by the illustration detector (i.e. it did not miss them altogether).\n",
    "\n",
    "The performance of our retrained illustration detector is as follows. Note that the performance metrics (AP and AR) value may differ (e.g. AP=0.832, AR=0.831) slightly between different runs. What do you think may be the reason for these differences?\n",
    "\n",
    "```\n",
    "AP = 0.841\n",
    "AR = 0.838 \n",
    "```\n",
    "\n",
    "This is a remarkably good level of performance obtained from just 25 training samples; the [researchers](https://arxiv.org/abs/1911.09070) who developed the EfficientDet model should be thanked for creating such a light weight, high performing model and sharing this model as an open source project that has enabled projects like the [VGG Chapbooks project](https://www.robots.ox.ac.uk/~vgg/research/chapbooks/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUDRpNpdUZ2I"
   },
   "source": [
    "### 3.4 Visualise Results from Newly Trained Illustration Detector\n",
    "\n",
    "We test the newly trained book illustration detector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kUOVtu9HZv-j",
    "outputId": "2caeb63a-2285-4966-9e73-fa10106ba535"
   },
   "outputs": [],
   "source": [
    "## Download test image\n",
    "image_url =  'https://raw.githubusercontent.com/gbergel/chapbooks-sagemaker-lab/gbergel-patch-3/Wynken.jpg' #@param\n",
    "test_image2_filename = 'test_image2.jpg'\n",
    "test_image2_path = os.path.join(DEMO_DIR, 'test_image2.jpg')\n",
    "!wget {image_url} -O {test_image2_path}\n",
    "\n",
    "show_thumbnail(test_image2_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfxEXdqrhFdX"
   },
   "source": [
    "Next, we convert the newly trained book illustration detector in a format (i.e. [saved model format](https://www.tensorflow.org/guide/saved_model)) that allows the detector to run at faster speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ky2XVNOJa7g6",
    "outputId": "73de6286-d8c5-4b0b-c7fe-91e62041a098"
   },
   "outputs": [],
   "source": [
    "## Convert model to saved-model format (for faster inference)\n",
    "os.chdir(BASEDIR + '/nls-chapbooks-illustrations/automl/efficientdet/')\n",
    "!PYTHONPATH={BASEDIR}/nls-chapbooks-illustrations/automl/efficientdet/ \\\n",
    "  python model_inspect.py \\\n",
    "  --runmode=saved_model \\\n",
    "  --model_name=efficientdet-d0 \\\n",
    "  --ckpt_path={MODEL_DIR} \\\n",
    "  --saved_model_dir={MODEL_DIR}/savedmodel \\\n",
    "  --hparams=\"num_classes=1,moving_average_decay=0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7VvpX_KhYA1"
   },
   "source": [
    "Next, we apply the book illustration detector on the downloaded test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IFfHMGK8Zv-k",
    "outputId": "d9766b41-abb8-4a3c-b74e-f2ef34eff593"
   },
   "outputs": [],
   "source": [
    "## Apply illustration detector to test image\n",
    "os.chdir(BASEDIR + '/nls-chapbooks-illustrations/tools')\n",
    "!PYTHONPATH={BASEDIR}/nls-chapbooks-illustrations/automl/efficientdet/ \\\n",
    "  python detect-illustration.py \\\n",
    "  --model-name=efficientdet-d0 \\\n",
    "  --saved-model-dir={MODEL_DIR}/savedmodel/  \\\n",
    "  --hparams={BASEDIR}/nls-chapbooks-illustrations/data/efficientdet/hparams.yaml \\\n",
    "  --input-image={test_image2_path} \\\n",
    "  --output-image-dir={DET_DIR} \\\n",
    "  --output-json-fn={DET_DIR}/metadata.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5qUov--hhds"
   },
   "source": [
    "Finally, we visualise the detection result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4BL6QAFzhllC",
    "outputId": "b36ed7d8-cde5-4c16-a856-306c2909e0f3"
   },
   "outputs": [],
   "source": [
    "## Show detection result\n",
    "show_thumbnail( os.path.join(DET_DIR, 'test_image2.jpg'), 1200 )\n",
    "with open( os.path.join(DET_DIR, 'metadata.json'), 'r' ) as f:\n",
    "  d = json.load(f)\n",
    "  print( json.dumps(d, indent=4) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmq5Xp8IiwNk",
    "tags": []
   },
   "source": [
    "### 3.5 What can be a challenging test image for this illustration detector trained on only 25 images?\n",
    "\n",
    "Deep learning models often face a challenge when they have to operate on test data that are different from the training data. Since we trained on 25 images taken from the NLS Chapbooks dataset, the [following image](https://gitlab.com/vgg/nls-chapbooks-illustrations/-/blob/master/data/images/test_images/BL_tyndales-new-testament-1526-c_188_a_17_f001r.jpg) taken from the [British Library](https://www.bl.uk/sacred-texts/articles/from-sacred-scriptures-to-the-peoples-bible) is a challenging test image for this model. Learners are encouraged to apply this illustration detector on this challenging image which may dishearten some of the learners. Here are some notes to help the learners think through this new observation.\n",
    "\n",
    "*   Each detection comes with a confidence level (0% to 100%) and if we only want to retain high confidence detections, we can set a high threshold (e.g. 0.9) in order to discard incorrect detection like the second detection (confidence = 0.81) which corresponds to a part of the illustration.\n",
    "*   Recall that we have trained this book illustration detector using only 25 instances of book illustration. More training samples may help improve the performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9vJ6QmbCERp"
   },
   "source": [
    "## 4. Additional Learning Exercise (Optional)\n",
    "Here is an extra challenge. How much performance improvement can we obtain by training on more samples? For example, what performance improvements will we obtain if we double the number of training samples to 50? What about training on 100 samples?\n",
    "\n",
    "We have in fact created datasets containing 50 and 100 manually annotated images while retaining the same 50 test images. Since the test images remains same, we can compare the performance improvement obtained by increasing the number of training samples. Note that the none of test images are contained in the training dataset. Here is the download link for these additional training datasets.\n",
    "\n",
    "*   [nls-chapbooks-50.zip](https://www.robots.ox.ac.uk/~vgg/research/chapbooks/dh2022/data/nls-chapbooks-50.zip) (12MB)\n",
    "  \n",
    "  - contains 50 manually annotated instances of book illustration\n",
    "  - test set contains 50 instances which is same as the test instances contained in nls-chapbooks-25.zip and nls-chapbooks-100.zip datasets.\n",
    "\n",
    "*   [nls-chapbooks-100.zip](https://www.robots.ox.ac.uk/~vgg/research/chapbooks/dh2022/data/nls-chapbooks-100.zip) (18MB)\n",
    "\n",
    "  - contains 100 manually annotated instances of book illustration\n",
    "  - test set contains 50 instances which is same as the test instances contained in nls-chapbooks-25.zip and nls-chapbooks-50.zip.\n",
    "\n",
    "Here are the steps you may want to follow in order to run experiments on these two datasets.\n",
    "\n",
    "1.   Download the new dataset\n",
    "2.   Convert to COCO format\n",
    "3.   Convert to tfrecord format\n",
    "4.   Start the training ensuring that settings are consistent across experiments\n",
    "\n",
    "\n",
    "Here are some questions that you can think about before starting the experiment?\n",
    "\n",
    "*   Will the performance improve?\n",
    "*   By how much (e.g. 1% or 5% or 10%) will the performance improve when the number of training samples first increases by 25 (i.e. number of training sample is 50) and then increases by 75?\n",
    "\n",
    "When you have the performance metrics, you can plot them using the following code by updating the second and third numbers in the `AP` and `AR` variables based on your experiments. Currently, it shows a flat line because we have replicated the same AP and AR performance metric for all the three cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "ko4SUdR9Z5Rw",
    "outputId": "84497d95-5c8c-4241-f4bf-c7538c367cea"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "AP = [0.841, 0.871, 0.891]  # update the 2nd and 3rd numbers based on your experiments\n",
    "AR = [0.838, 0.858, 0.878]  # update the 2nd and 3rd numbers based on your experiments\n",
    "TRAINING_IMAGE_COUNT = [25, 50, 100]\n",
    "\n",
    "plt.plot(TRAINING_IMAGE_COUNT, AP, color='#0072B2', marker='o', label='Average Precision (AP)')\n",
    "plt.plot(TRAINING_IMAGE_COUNT, AR, color='#D55E00', marker='o', label='Average Recall (AR)')\n",
    "plt.xlabel('Number of training images')\n",
    "plt.ylabel('Detection performance (AP and AR)')\n",
    "plt.title('Book Illustration Detection Performance Dependence on Number of Training Image')\n",
    "#plt.ylim(0.7, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJWN9cbQ_zlq"
   },
   "source": [
    "## 5. Poll\n",
    "What other types of automatic detectors can you think of that may be useful for digital humanists/book historians? Do share your thoughts with fellow learners during the workshop, or afterwards.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBaINgN--uHw"
   },
   "source": [
    "# 6. Frequently Asked Questions (FAQ)\n",
    "\n",
    "* I got an error stating that something was not found or not defined (e.g. `NameError: name 'os' is not defined`)\n",
    "> Most likely, you are executing a command cell with executing the previous cells which contains some dependencies (e.g. import statements, generate folders, etc.) that are used by the cell. Did you run the commands given in section \"1. Download and Install the Required Tools\" ?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": " Visual Analysis of Chapbooks Printed in Scotland.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
